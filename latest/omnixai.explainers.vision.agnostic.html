<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>omnixai.explainers.vision.agnostic package &mdash; OmniXAI  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="omnixai.explainers.vision.specific package" href="omnixai.explainers.vision.specific.html" />
    <link rel="prev" title="omnixai.explainers.vision package" href="omnixai.explainers.vision.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> OmniXAI
          </a>
              <div class="version">
                latest
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="omnixai.html">OmniXAI: An Explanation Toolbox</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#getting-started">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#how-to-contribute">How to Contribute</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#modules-for-different-data-types">Modules for Different Data Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#preprocessing-functions">Preprocessing Functions</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="omnixai.html#supported-explanation-methods">Supported Explanation Methods</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="omnixai.explainers.html">omnixai.explainers package</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="omnixai.explainers.html#module-omnixai.explainers.base">omnixai.explainers.base module</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="omnixai.explainers.html#explainers-for-different-tasks">Explainers for different tasks</a><ul class="current">
<li class="toctree-l5"><a class="reference internal" href="omnixai.explainers.data.html">omnixai.explainers.data package</a></li>
<li class="toctree-l5"><a class="reference internal" href="omnixai.explainers.tabular.html">omnixai.explainers.tabular package</a></li>
<li class="toctree-l5 current"><a class="reference internal" href="omnixai.explainers.vision.html">omnixai.explainers.vision package</a><ul class="current">
<li class="toctree-l6"><a class="reference internal" href="omnixai.explainers.vision.html#module-omnixai.explainers.vision.auto">omnixai.explainers.vision.auto module</a></li>
<li class="toctree-l6 current"><a class="reference internal" href="omnixai.explainers.vision.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l7 current"><a class="current reference internal" href="#">omnixai.explainers.vision.agnostic package</a><ul>
<li class="toctree-l8"><a class="reference internal" href="#module-omnixai.explainers.vision.agnostic.lime">omnixai.explainers.vision.agnostic.lime module</a></li>
<li class="toctree-l8"><a class="reference internal" href="#module-omnixai.explainers.vision.agnostic.shap">omnixai.explainers.vision.agnostic.shap module</a></li>
<li class="toctree-l8"><a class="reference internal" href="#module-omnixai.explainers.vision.agnostic.pdp">omnixai.explainers.vision.agnostic.pdp module</a></li>
<li class="toctree-l8"><a class="reference internal" href="#module-omnixai.explainers.vision.agnostic.l2x">omnixai.explainers.vision.agnostic.l2x module</a></li>
</ul>
</li>
<li class="toctree-l7"><a class="reference internal" href="omnixai.explainers.vision.specific.html">omnixai.explainers.vision.specific package</a></li>
<li class="toctree-l7"><a class="reference internal" href="omnixai.explainers.vision.counterfactual.html">omnixai.explainers.vision.counterfactual package</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="omnixai.explainers.nlp.html">omnixai.explainers.nlp package</a></li>
<li class="toctree-l5"><a class="reference internal" href="omnixai.explainers.timeseries.html">omnixai.explainers.timeseries package</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#modules-for-explanation-results">Modules for Explanation Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#dashboard-for-visualization">Dashboard for Visualization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials &amp; Example Code</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">OmniXAI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="omnixai.html">OmniXAI: An Explanation Toolbox</a> &raquo;</li>
          <li><a href="omnixai.explainers.html">omnixai.explainers package</a> &raquo;</li>
          <li><a href="omnixai.explainers.vision.html">omnixai.explainers.vision package</a> &raquo;</li>
      <li>omnixai.explainers.vision.agnostic package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/omnixai.explainers.vision.agnostic.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="module-omnixai.explainers.vision.agnostic">
<span id="omnixai-explainers-vision-agnostic-package"></span><h1>omnixai.explainers.vision.agnostic package<a class="headerlink" href="#module-omnixai.explainers.vision.agnostic" title="Permalink to this heading"></a></h1>
<table class="autosummary longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#module-omnixai.explainers.vision.agnostic.lime" title="omnixai.explainers.vision.agnostic.lime"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lime</span></code></a></p></td>
<td><p>The LIME explainer for image classification.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#module-omnixai.explainers.vision.agnostic.shap" title="omnixai.explainers.vision.agnostic.shap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">shap</span></code></a></p></td>
<td><p>The SHAP explainer for vision tasks.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#module-omnixai.explainers.vision.agnostic.pdp" title="omnixai.explainers.vision.agnostic.pdp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pdp</span></code></a></p></td>
<td><p>The partial dependence plots for vision tasks.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#module-omnixai.explainers.vision.agnostic.l2x" title="omnixai.explainers.vision.agnostic.l2x"><code class="xref py py-obj docutils literal notranslate"><span class="pre">l2x</span></code></a></p></td>
<td><p>The L2X explainer for image data.</p></td>
</tr>
</tbody>
</table>
<section id="module-omnixai.explainers.vision.agnostic.lime">
<span id="omnixai-explainers-vision-agnostic-lime-module"></span><h2>omnixai.explainers.vision.agnostic.lime module<a class="headerlink" href="#module-omnixai.explainers.vision.agnostic.lime" title="Permalink to this heading"></a></h2>
<p>The LIME explainer for image classification.</p>
<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.lime.LimeImage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.agnostic.lime.</span></span><span class="sig-name descname"><span class="pre">LimeImage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">predict_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.agnostic.lime.LimeImage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.ExplainerBase" title="omnixai.explainers.base.ExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExplainerBase</span></code></a></p>
<p>The LIME explainer for image classification.
If using this explainer, please cite the original work: <a class="reference external" href="https://github.com/marcotcr/lime">https://github.com/marcotcr/lime</a>.
This explainer only supports image classification.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predict_function</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>) – The prediction function corresponding to the machine learning
model to explain. For classification, the outputs of the <code class="docutils literal notranslate"><span class="pre">predict_function</span></code>
are the class probabilities.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The task type can be <cite>classification</cite> only.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.lime.LimeImage.explanation_type">
<span class="sig-name descname"><span class="pre">explanation_type</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'local'</span></em><a class="headerlink" href="#omnixai.explainers.vision.agnostic.lime.LimeImage.explanation_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.lime.LimeImage.alias">
<span class="sig-name descname"><span class="pre">alias</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['lime']</span></em><a class="headerlink" href="#omnixai.explainers.vision.agnostic.lime.LimeImage.alias" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.lime.LimeImage.explain">
<span class="sig-name descname"><span class="pre">explain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.agnostic.lime.LimeImage.explain" title="Permalink to this definition"></a></dt>
<dd><p>Generates the explanations for the input instances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> – A batch of input instances.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters, e.g., <code class="docutils literal notranslate"><span class="pre">top_labels</span></code> – the number
of the top labels to explain. Please refer to the doc of
<cite>LimeImageExplainer.explain_instance</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="omnixai.explanations.image.html#omnixai.explanations.image.mask.MaskExplanation" title="omnixai.explanations.image.mask.MaskExplanation"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaskExplanation</span></code></a></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The explanations for all the input instances.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-omnixai.explainers.vision.agnostic.shap">
<span id="omnixai-explainers-vision-agnostic-shap-module"></span><h2>omnixai.explainers.vision.agnostic.shap module<a class="headerlink" href="#module-omnixai.explainers.vision.agnostic.shap" title="Permalink to this heading"></a></h2>
<p>The SHAP explainer for vision tasks.</p>
<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.shap.ShapImage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.agnostic.shap.</span></span><span class="sig-name descname"><span class="pre">ShapImage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocess_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">background_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.agnostic.shap.ShapImage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.ExplainerBase" title="omnixai.explainers.base.ExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExplainerBase</span></code></a></p>
<p>The SHAP explainer for vision tasks.
If using this explainer, please cite the original work: <a class="reference external" href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The model to explain, whose type can be <cite>tf.keras.Model</cite> or <cite>torch.nn.Module</cite>.</p></li>
<li><p><strong>preprocess_function</strong> – The preprocessing function that converts the raw input features
into the inputs of <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The task type, e.g., <cite>classification</cite> or <cite>regression</cite>.</p></li>
<li><p><strong>background_data</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – The background images to compare with.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.shap.ShapImage.explanation_type">
<span class="sig-name descname"><span class="pre">explanation_type</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'local'</span></em><a class="headerlink" href="#omnixai.explainers.vision.agnostic.shap.ShapImage.explanation_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.shap.ShapImage.alias">
<span class="sig-name descname"><span class="pre">alias</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['shap']</span></em><a class="headerlink" href="#omnixai.explainers.vision.agnostic.shap.ShapImage.alias" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.shap.ShapImage.explain">
<span class="sig-name descname"><span class="pre">explain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.agnostic.shap.ShapImage.explain" title="Permalink to this definition"></a></dt>
<dd><p>Generates the pixel-importance explanations for the input instances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – A batch of input instances.</p></li>
<li><p><strong>y</strong> – A batch of labels to explain. For regression, <code class="docutils literal notranslate"><span class="pre">y</span></code> is ignored.
For classification, the top predicted label of each input instance will be explained
when <cite>y = None</cite>.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters, e.g., <code class="docutils literal notranslate"><span class="pre">nsamples</span></code> – the maximum number of images
sampled for the background.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="omnixai.explanations.image.html#omnixai.explanations.image.pixel_importance.PixelImportance" title="omnixai.explanations.image.pixel_importance.PixelImportance"><code class="xref py py-class docutils literal notranslate"><span class="pre">PixelImportance</span></code></a></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The explanations for all the input instances, e.g., pixel importance scores.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-omnixai.explainers.vision.agnostic.pdp">
<span id="omnixai-explainers-vision-agnostic-pdp-module"></span><h2>omnixai.explainers.vision.agnostic.pdp module<a class="headerlink" href="#module-omnixai.explainers.vision.agnostic.pdp" title="Permalink to this heading"></a></h2>
<p>The partial dependence plots for vision tasks.</p>
<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.pdp.PartialDependenceImage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.agnostic.pdp.</span></span><span class="sig-name descname"><span class="pre">PartialDependenceImage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">predict_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.agnostic.pdp.PartialDependenceImage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.ExplainerBase" title="omnixai.explainers.base.ExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExplainerBase</span></code></a></p>
<p>The partial dependence plots for vision tasks. The input image is segmented by a particular
segmentation method, e.g., “quickshift”. For each segment, its importance score is measured
by the average change of the predicted value when the segment is replaced by new segments constructed
in the grid search.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predict_function</strong> – The prediction function corresponding to the model to explain.
When the model is for classification, the outputs of the <code class="docutils literal notranslate"><span class="pre">predict_function</span></code>
are the class probabilities. When the model is for regression, the outputs of
the <code class="docutils literal notranslate"><span class="pre">predict_function</span></code> are the estimated values.</p></li>
<li><p><strong>mode</strong> – The task type, e.g., <cite>classification</cite> or <cite>regression</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.pdp.PartialDependenceImage.explanation_type">
<span class="sig-name descname"><span class="pre">explanation_type</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'local'</span></em><a class="headerlink" href="#omnixai.explainers.vision.agnostic.pdp.PartialDependenceImage.explanation_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.pdp.PartialDependenceImage.alias">
<span class="sig-name descname"><span class="pre">alias</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['pdp',</span> <span class="pre">'partial_dependence']</span></em><a class="headerlink" href="#omnixai.explainers.vision.agnostic.pdp.PartialDependenceImage.alias" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.pdp.PartialDependenceImage.explain">
<span class="sig-name descname"><span class="pre">explain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.agnostic.pdp.PartialDependenceImage.explain" title="Permalink to this definition"></a></dt>
<dd><p>Generates PDP explanations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – A batch of input instances.</p></li>
<li><p><strong>y</strong> – A batch of labels to explain. For regression, <code class="docutils literal notranslate"><span class="pre">y</span></code> is ignored.
For classification, the top predicted label of each input instance will be explained
when <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">None</span></code>.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters in the PDP explainer, e.g., <code class="docutils literal notranslate"><span class="pre">grid_resolution</span></code> –
the resolution in the grid search, and <code class="docutils literal notranslate"><span class="pre">n_segments</span></code> – the number of image segments used
by image segmentation methods.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="omnixai.explanations.image.html#omnixai.explanations.image.pixel_importance.PixelImportance" title="omnixai.explanations.image.pixel_importance.PixelImportance"><code class="xref py py-class docutils literal notranslate"><span class="pre">PixelImportance</span></code></a></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The generated explanations, e.g., the importance scores for image segments.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-omnixai.explainers.vision.agnostic.l2x">
<span id="omnixai-explainers-vision-agnostic-l2x-module"></span><h2>omnixai.explainers.vision.agnostic.l2x module<a class="headerlink" href="#module-omnixai.explainers.vision.agnostic.l2x" title="Permalink to this heading"></a></h2>
<p>The L2X explainer for image data.</p>
<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.l2x.DefaultSelectionModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.agnostic.l2x.</span></span><span class="sig-name descname"><span class="pre">DefaultSelectionModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">explainer</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.agnostic.l2x.DefaultSelectionModel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">_DefaultModelBase</span></code></p>
<p>The default selection model in L2X, which is designed for MNIST.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>explainer</strong> – A <cite>L2XImage</cite> explainer.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.l2x.DefaultSelectionModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.agnostic.l2x.DefaultSelectionModel.forward" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – The model inputs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.l2x.DefaultSelectionModel.postprocess">
<span class="sig-name descname"><span class="pre">postprocess</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.agnostic.l2x.DefaultSelectionModel.postprocess" title="Permalink to this definition"></a></dt>
<dd><p>Upsamples to the original image size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – The outputs of <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.l2x.DefaultSelectionModel.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#omnixai.explainers.vision.agnostic.l2x.DefaultSelectionModel.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.l2x.DefaultPredictionModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.agnostic.l2x.</span></span><span class="sig-name descname"><span class="pre">DefaultPredictionModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">explainer</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.agnostic.l2x.DefaultPredictionModel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">_DefaultModelBase</span></code></p>
<p>The default prediction model in L2X, which is designed for MNIST.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>explainer</strong> – A <cite>L2XImage</cite> explainer.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.l2x.DefaultPredictionModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.agnostic.l2x.DefaultPredictionModel.forward" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – The model inputs.</p></li>
<li><p><strong>weights</strong> – The weights generated via Gumbel-Softmax sampling.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.l2x.DefaultPredictionModel.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#omnixai.explainers.vision.agnostic.l2x.DefaultPredictionModel.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.l2x.L2XImage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.agnostic.l2x.</span></span><span class="sig-name descname"><span class="pre">L2XImage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">training_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selection_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_function</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.agnostic.l2x.L2XImage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.ExplainerBase" title="omnixai.explainers.base.ExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExplainerBase</span></code></a></p>
<p>The LIME explainer for vision tasks.
If using this explainer, please cite the original work:
<cite>Learning to Explain: An Information-Theoretic Perspective on Model Interpretation,
Jianbo Chen, Le Song, Martin J. Wainwright, Michael I. Jordan, https://arxiv.org/abs/1802.07814</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>training_data</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – The data used to train the explainer. <code class="docutils literal notranslate"><span class="pre">training_data</span></code>
should be the training dataset for training the machine learning model.</p></li>
<li><p><strong>predict_function</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>) – The prediction function corresponding to the model to explain.
When the model is for classification, the outputs of the <code class="docutils literal notranslate"><span class="pre">predict_function</span></code>
are the class probabilities. When the model is for regression, the outputs of
the <code class="docutils literal notranslate"><span class="pre">predict_function</span></code> are the estimated values.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The task type, e.g., <cite>classification</cite> or <cite>regression</cite>.</p></li>
<li><p><strong>tau</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Parameter <code class="docutils literal notranslate"><span class="pre">tau</span></code> in Gumbel-Softmax.</p></li>
<li><p><strong>k</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The maximum number of the selected features in L2X.</p></li>
<li><p><strong>selection_model</strong> – A pytorch model class for estimating P(S|X) in L2X. If
<code class="docutils literal notranslate"><span class="pre">selection_model</span> <span class="pre">=</span> <span class="pre">None</span></code>, a default model <cite>DefaultSelectionModel</cite> will be used.</p></li>
<li><p><strong>prediction_model</strong> – A pytorch model class for estimating Q(X_S) in L2X. If
<code class="docutils literal notranslate"><span class="pre">prediction_model</span> <span class="pre">=</span> <span class="pre">None</span></code>, a default model <cite>DefaultPredictionModel</cite> will be used.</p></li>
<li><p><strong>loss_function</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – The loss function for the task, e.g., <cite>nn.CrossEntropyLoss()</cite>
for classification.</p></li>
<li><p><strong>optimizer</strong> – The optimizer class for training the explainer, e.g., <cite>torch.optim.Adam</cite>.</p></li>
<li><p><strong>learning_rate</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – The learning rate for training the explainer.</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The batch size for training the explainer. If <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is <cite>None</cite>,
<code class="docutils literal notranslate"><span class="pre">batch_size</span></code> will be picked from <cite>[32, 64, 128, 256]</cite> based on the sample size.</p></li>
<li><p><strong>num_epochs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The number of epochs for training the explainer.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters, e.g., parameters for <code class="docutils literal notranslate"><span class="pre">selection_model</span></code>
and <code class="docutils literal notranslate"><span class="pre">prediction_model</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.l2x.L2XImage.explanation_type">
<span class="sig-name descname"><span class="pre">explanation_type</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'local'</span></em><a class="headerlink" href="#omnixai.explainers.vision.agnostic.l2x.L2XImage.explanation_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.l2x.L2XImage.alias">
<span class="sig-name descname"><span class="pre">alias</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['l2x',</span> <span class="pre">'L2X']</span></em><a class="headerlink" href="#omnixai.explainers.vision.agnostic.l2x.L2XImage.alias" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.agnostic.l2x.L2XImage.explain">
<span class="sig-name descname"><span class="pre">explain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.agnostic.l2x.L2XImage.explain" title="Permalink to this definition"></a></dt>
<dd><p>Generates the explanations for the input instances. For classification,
it explains the top predicted label for each input instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – A batch of input instances.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="omnixai.explanations.image.html#omnixai.explanations.image.pixel_importance.PixelImportance" title="omnixai.explanations.image.pixel_importance.PixelImportance"><code class="xref py py-class docutils literal notranslate"><span class="pre">PixelImportance</span></code></a></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The explanations for all the input instances.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="omnixai.explainers.vision.html" class="btn btn-neutral float-left" title="omnixai.explainers.vision package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="omnixai.explainers.vision.specific.html" class="btn btn-neutral float-right" title="omnixai.explainers.vision.specific package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, salesforce.com, inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Versions</span>
      latest
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      
      <dl>
        <dt>Versions</dt>
        
           <strong> 
          <dd><a href="">latest</a></dd>
           </strong> 
        
      </dl>
      
    </div>
  </div>

 <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>